% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/text_core.R
\name{SentencePieceTokenizer}
\alias{SentencePieceTokenizer}
\title{SentencePieceTokenizer}
\usage{
SentencePieceTokenizer(
  lang = "en",
  special_toks = NULL,
  sp_model = NULL,
  vocab_sz = NULL,
  max_vocab_sz = 30000L,
  model_type = "unigram",
  char_coverage = NULL,
  cache_dir = "tmp"
)
}
\arguments{
\item{lang}{lang}

\item{special_toks}{special_toks}

\item{sp_model}{sp_model}

\item{vocab_sz}{vocab_sz}

\item{max_vocab_sz}{max_vocab_sz}

\item{model_type}{model_type}

\item{char_coverage}{char_coverage}

\item{cache_dir}{cache_dir}
}
\description{
SentencePiece tokenizer for `lang`
}
\details{

}
