% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fastaibuilt.R
\name{RetinaNetFocalLoss}
\alias{RetinaNetFocalLoss}
\title{RetinaNetFocalLoss}
\usage{
RetinaNetFocalLoss(
  gamma = 2,
  alpha = 0.25,
  pad_idx = 0,
  scales = NULL,
  ratios = NULL,
  reg_loss = nn$functional$smooth_l1_loss
)
}
\arguments{
\item{gamma}{gamma}

\item{alpha}{alpha}

\item{pad_idx}{pad_idx}

\item{scales}{scales}

\item{ratios}{ratios}

\item{reg_loss}{reg_loss}
}
\description{
Base class for all neural network modules.
}
\details{
Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes:: import torch.nn as nn import torch.nn.functional as F class Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:`to`, etc.
}
