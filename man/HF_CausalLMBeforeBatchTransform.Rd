% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/blurr_hugging_face.R
\name{HF_CausalLMBeforeBatchTransform}
\alias{HF_CausalLMBeforeBatchTransform}
\title{HF_CausalLMBeforeBatchTransform}
\usage{
HF_CausalLMBeforeBatchTransform(
  hf_arch,
  hf_tokenizer,
  max_length = NULL,
  padding = TRUE,
  truncation = TRUE,
  is_split_into_words = FALSE,
  n_tok_inps = 1,
  ignore_token_id = -100,
  ...
)
}
\arguments{
\item{hf_arch}{architecture}

\item{hf_tokenizer}{tokenizer}

\item{max_length}{maximum length}

\item{padding}{padding or not}

\item{truncation}{truncation or not}

\item{is_split_into_words}{to split into words}

\item{n_tok_inps}{number tok inputs}

\item{ignore_token_id}{ignore token id}

\item{...}{additional arguments}
}
\value{
None
}
\description{
Handles everything you need to assemble a mini-batch of inputs and targets,
as well as decode the dictionary produced
}
\details{
as a byproduct of the tokenization process in the `encodes` method.
}
